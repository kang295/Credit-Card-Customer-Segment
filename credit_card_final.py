# -*- coding: utf-8 -*-
"""Credit_Card_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AXJEVg6SyFE7sSt4nqSpiwyx26ObcWab

**Project Objective**: Utilize a Clustering Algorithm to Segment Credit Card Customers, analyze each cluster, and develop efficient marketing strategies based on the analysis.

**Project Description**: Utilize a sample dataset comprising the recent 6-month usage behavior of approximately 9000 active credit card holders. The dataset is structured at the customer level and encompasses 18 behavioral variables.

**Data Source**: https://www.kaggle.com/datasets/arjunbhasin2013/ccdata

**Project Collaborator**: Sewon Hong

# Data Import and Preprocessing
"""

# Import the relevant libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

# Importing the dataset

data = pd.read_csv('/content/sample_data/CC GENERAL.csv')

# Quick overview of our dataset

data.head(5)

# Dataset has 8950 rows and 18 features

data.shape

# Some features have null-values ('CREDIT_LIMIT', 'MINIMUM_PAYMENTS')
# Those values have to be taken care of before building a ML model for better performance.

data.info()

# By looking at the quick statistics, we can get insights about the dataset.

data.describe()

"""1. The average balance across 8950 customers are approximately $1564, reflecting the typical credit amount held in their accounts.

2. People tend to update their balance very often by approximately 88%.

3. The average purchase amount is about $1000 across all the customers.

4. Average single-time purchase by customers is $592, showing occasional substantial purchase behavior.

5. Installment-based purchases, averaging around $411, represent a significant portion of customer spending (40%).
"""

data.isnull().sum()

# No duplicated rows

data.duplicated().sum()

# Since we have relatively few number of rows for 'MINIMUM_PAYMENTS', just fill those rows with median values.

data['MINIMUM_PAYMENTS'] = data['MINIMUM_PAYMENTS'].fillna(data['MINIMUM_PAYMENTS'].median())

# Here, single row with null-values in 'CREDIT_LIMIT' is just dropped since it does not affect our analysis that much.

data.dropna(inplace=True)
data.isnull().sum()

# Also, 'CUST_ID' is non necessary for analysis since it has nothing important in its value.

data.drop(columns='CUST_ID', inplace=True)

# This histograms are made to see the overview of each feature's distribution.
# Most of the distribution is right skewed (this should be fixed to have normal distribution)
# since K-means algorithm assume that the data is normally distributed or at least symmetric.

fig = plt.figure(figsize = (20,35))

for i, col in enumerate(data.columns):
  ax = plt.subplot(6, 3, i+1)
  sns.histplot(data, x = col, ax = ax, color = 'green', kde = True, alpha = 0.2)
  plt.xlabel(col, fontsize = 15)
  plt.ylabel("Count", fontsize = 15)
  plt.xticks(fontsize = 13)
  plt.yticks(fontsize = 13)
plt.tight_layout()
fig.show()

"""## Now generate box plots for each features to see the outliers

* Using the Interquartile Range (IQR) method to identify and potentially remove outliers is a common preprocessing step in data analysis, including before building machine learning models. Outliers can skew statistical analyses and model predictions, leading to inaccurate results. Therefore, it's often beneficial to address outliers before proceeding with model building
"""

columns_list = data.columns.tolist()

plt.rcParams['figure.figsize'] = [20,15]

sns.boxplot(data=data[columns_list], orient='v', palette='Set2', whis=1.5, saturation=1, width=0.7)
plt.title("Outliers Variable Distribution", fontsize = 14, fontweight = 'bold')
plt.ylabel("Range", fontweight = 'bold')
plt.xlabel("Attributes", fontweight = 'bold')
plt.xticks(rotation=90)

# Now, we have only 8588 rows after getting rid of extreme outliers filtered by IQR method.

columns_to_process = ['BALANCE', 'PURCHASES', 'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE', 'CREDIT_LIMIT', 'PAYMENTS', 'MINIMUM_PAYMENTS']

for column in columns_to_process:
    Q1 = data[column].quantile(0.05)
    Q3 = data[column].quantile(0.95)
    IQR = Q3 - Q1
    data = data[(data[column] >= Q1 - 1.5*IQR) & (data[column] <= Q3 + 1.5*IQR)]

data.shape

"""## By looking at the heatmap, we can see how different features are correlated with each other.

1. People who frequently make many purchases often also tend to make numerous installment payments.
2. People with higher balances tend to have larger advance cash rate.
3. Customers with higher balances tend to have higher minimum payments
"""

# Showing only highly correlated (>0.6) feature relationships

corr_matrix = data.corr()
mask = np.abs(corr_matrix) < 0.6
fig = plt.figure(figsize=(20, 10))
sns.heatmap(corr_matrix, cmap='Reds', square=True, annot=True, linewidths=0.5, mask=mask)
plt.show()

# Filtering the columns that need to be normalized

cols = ['BALANCE', 'PURCHASES', 'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE', 'CASH_ADVANCE_TRX', 'PURCHASES_TRX',
       'CREDIT_LIMIT', 'PAYMENTS', 'MINIMUM_PAYMENTS']

"""### Reasons for applying Normalization (Sqrt)
**Reduce Skewness**: It can help in reducing the skewness of positively skewed data, making the distribution more symmetric and closer to a normal distribution. This is particularly beneficial because many algorithms, including K-means, assume that the data is normally distributed or at least symmetric.

**Handle Outliers**: Applying a square root transformation can also mitigate the effect of outliers since it reduces the range and variability of higher values more than lower values.

**Scaling**: It provides a form of scaling that can be more appropriate for certain data distributions than standard scaling methods like Min-Max scaling or Z-score normalization.
"""

# Function for Normalization

def feature_engi(column):
  column = np.sqrt(column)
  return column

for col in cols:
  data[col] = feature_engi(data[col])

# Let's check if the distibution of histogram is close to normal distribution.

fig = plt.figure(figsize = (20,35))
for i, col in enumerate(cols):
  ax = plt.subplot(6, 3, i+1)
  sns.histplot(data, x = col, ax = ax, color = 'green', kde = True, alpha = 0.2)
  plt.xlabel(col, fontsize = 15)
  plt.ylabel("Count", fontsize = 15)
  plt.xticks(fontsize = 13)
  plt.yticks(fontsize = 13)
plt.tight_layout()
fig.show()

"""We can see that there data is less right skwed"""

# Normalized dataset

data.head(5)

"""# Model Building"""

# Importing relevant libraries

import sklearn
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer

"""## Reasons for Applying StandardScaler After Normalization
**Uniformity Across Features**: Even after normalization (such as square root transformation), features might operate on different scales. StandardScaler ensures that all features contribute equally to the distance computation, which is crucial for algorithms like K-means that rely on distance measures to form clusters.

**Optimization for Algorithms**: PCA performs better or converge faster when the data is on the same scale. PCA, for instance, is affected by the scale of the features since it looks for directions of maximizing variance. If one feature has a very large variance and others do not, PCA will be heavily biased towards those features with large variances.

**Enhanced Interpretability**: Standardizing the data after normalization can make the results more interpretable, especially in PCA, where you're looking at component weights to understand the importance of each feature in the components.

**Handling Residual Skewness**: Even after normalization, there might still be some residual skewness or discrepancies in the distribution of the features. Standardizing can help mitigate any remaining inconsistencies.
"""

# Applying standard scaler on normalized data

scaler = StandardScaler()
df_scaled = scaler.fit_transform(data)

df_scaled = pd.DataFrame(data = df_scaled, columns = data.columns)
df_scaled.head()

"""# PCA (Principal Component Analysis) before Clustering

Using PCA before K-means can significantly improve the clustering process and outcomes by reducing dimensionality, mitigating noise, enhancing visualization capabilities, and ensuring that the data structure is more compatible with the assumptions of the K-means algorithm.
"""

# Applying PCA algorithm to df_scaled

pca = PCA()
X_pca = pca.fit_transform(df_scaled)
X_pca

df_pca = pd.DataFrame(data = X_pca, columns = [f'PC{i+1}' for i in range(X_pca.shape[1])])
df_pca.head()

evr = list(pca.explained_variance_ratio_)
evr = sorted(evr, reverse = True)

evr_cum = list(pca.explained_variance_ratio_.cumsum())

# number of PCA components

num_components = [x+1 for x in range(X_pca.shape[1])]
num_components

"""**Cumulative Explained Variance**: 70-80% is often considered a good balance for retaining most of the important information in the data while achieving significant dimensionality reduction.

I will go with **5 components** based on the above information.
"""

# Finding the optimal number of PCA component that explains decent amount of information

fig, ax = plt.subplots(1, 2, figsize = (12, 5))

sns.barplot(x = num_components, y = evr, ax = ax[0])
ax[0].set_xlabel("Component")
ax[0].set_title("% Explained Variance")

sns.lineplot(x = num_components, y = evr_cum, marker = 'o' ,ax = ax[1])
ax[1].set_xlabel("Component")
ax[1].set_title("% Cumulative Variance")

fig.show()

# Re-defining the pca model and applying to df_scaled

pca = PCA(n_components = 5)
X_pca = pca.fit_transform(df_scaled)

df_pca = pd.DataFrame(data = X_pca, columns = [f'PC{i+1}' for i in range(X_pca.shape[1])])
df_pca.head()

"""# KMeans Clustering

**K-means clustering** is unsupervised machine learning algorithm used for partitioning a dataset into a predefined number of clusters.
The goal is to group data points that are similar to each other while minimizing the variance within each cluster.

Let's first investigate what is the optimal number of K by iterating the different k values from 1 to 10 in our model.
"""

kmeans_models = [KMeans(n_clusters = k, random_state = 42).fit(X_pca) for k in range(1,10)]

inertias = [model.inertia_ for model in kmeans_models]

fig = plt.figure(figsize = (6,4))
sns.lineplot(x = list(range(1,10)), y = inertias, marker = 'x')
plt.xlabel("Number of Clusters")
plt.ylabel('Within-Cluster Sum of Squares(Inertia)')
plt.show()

kmeans = KMeans()
vis_elbow = KElbowVisualizer(kmeans, k = (1, 10))
vis_elbow.fit(df_scaled)
vis_elbow.poof()

"""After the automated elbow method, it is concluded that k=3 is optimal number.

"""

# Applying K=3 to KMeans clustering using pca applied dataset

kmeans = KMeans(n_clusters = 3, random_state=0)

pca_labels = kmeans.fit_predict(X_pca)

pca_labels

# Checking the clustering result

plt.figure(figsize=(25,15))
sns.scatterplot(data = df_pca, x= "PC1", y= "PC2", hue = pca_labels, palette='viridis')
plt.xlabel("PC1", fontsize = 25)
plt.ylabel("PC2", fontsize = 25)
plt.legend(['Cluster 0', 'Cluster 1', 'Cluster 2'],fontsize = 25)

plt.show()

# reverting the feature values by squaring the values

def feature_engg(column):
    column = np.square(column)
    return column

for col in cols:
    data[col] = data[col].apply(feature_engg)

data['cluster'] = pca_labels
data.head()

"""# Cluster Analysis"""

# Checking the count of each cluster data points

sns.set(style="whitegrid")
palette = sns.color_palette("Set1")

ax = sns.countplot(x='cluster', data=data, palette=palette, label='Count')

for p in ax.patches:
    ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center',
                xytext=(0, 5), textcoords='offset points')

plt.show()

df_cols=data.columns
df_cols.tolist()

# Box plot results for each cluster by feature

plt.figure(figsize = (25, 25))
for i, col in enumerate(df_cols):
  if i+1 < len(df_cols):
    ax = plt.subplot(6,3, i+1)
    sns.boxplot(x = data['cluster'], y = data[col],hue=data['cluster'], palette='Set1')
    plt.xlabel("Cluster", fontsize = 15)
    plt.ylabel(col, fontsize = 15)
    plt.xticks(fontsize = 13)
    plt.yticks(fontsize = 13)


plt.tight_layout()
plt.show()

data.groupby('cluster').mean()

"""# **Cluster 0 - Cash Advancment Reliant Users**

#### These customers maintain a high balance, yet don't seem to make many purchases. Most of their balance appears to stem from cash advances, as evidenced by their highest rate in cash advance transactions, frequency, and the amount spent on them. Considering their lowest rate of full payment among all three clusters, it suggests they may not be fully committed to or capable of paying off their balances.
* High Balance
* Frequently update their balance
* Spend least money on purchases, one-off, installment purchases
* Get the most amount, the most frequent, and the most transactions of cash advancement
* Least purchase frequency on all 3 and least purchase transaction
* Moderate credit limit, moderate payments
* Maximum minimum payments
* Notably lowest for the percent of full payment

## **[To target these customers]**

#### Offering Installment Benefits: As these customers prefer cash services over purchases, incentivizing purchases through installment benefits can be effective. Card companies can enhance benefits for installment payments to encourage these customers to use their cards for purchases.

#### Providing Financial Education and Management Services: Since these customers may face difficulties in repaying credit card balances, providing financial education and management services can help address their financial issues. Additionally, offering balance management tools can support effective balance management and debt reduction.

# **Cluster 1 - Moderate Spenders**
#### These customers use credit cards a moderate amount, yet, with a low balance. They can be students or young professionals who have limited financial resources and may use credit cards cautiously. They may not update their balances frequently and tend to make moderate purchases without excessive spending. Or may include cards that are used as a second card. They may spend less money using the card, but they do so frequently, and the percentage of making a full payment is about 20%.
* Low Balance
* Balance is updated less frequently
* purchases, one-off, installment purchases all consists a moderate amount
* least spent on and least frequent in cash advance
* moderate frequency of purchases, including one-off and installment purchases
* moderate purchase transaction
* lowest payments, minimum payments
* The percentage of full payment is relatively high compared to cluster 0, but still low with below 20%

##**[To target these customers]**

####Highlighting savings and economic benefits can be effective for this segment, as they typically have low balances and limited financial resources. Offering promotions, discounts, and benefits for affordable products and services can emphasize the value of their purchases.

####Additionally, since this segment may not frequently update their balances and manage their spending carefully, offering practical and valuable additional services can enhance customer value. This could include budgeting tools or financial advisory services.

#### We can also offer them discounts on everyday expenses, such as coffee, meals, and educational-related fees.

# **Cluster 2 - Heavy Credit Card Users**
#### These customers utilize credit cards frequently, making both big amount of installment and one-time payments. They not only use credit cards extensively but also manage payments effectively, shown by a realtively high percentage of making a full payment. This segment demonstrates financial stability and possesses the capability to make substantial purchases.
* Moderate to high balance
* Frequently update their balance
* Highest purchase, including highest one-off and installment purchases
* Noticeably the highest in purchase, one-off, and installment purchase frequency
* Lower cash advance frequency, the amount spent on cash advance, and the transaction
* Highest purchase transactions
* Highest credit limit
* Highest payments, and a little lower than the cluster1 on minimum payments
* The percentage of full payment is relatively high compared to cluster 0, but still low with below 20%

##**[To target these customers]**

#### Consequently, there is an anticipation of high demand for shopping and consumption-related products and services within this segment. Promoting them with discounts or giving points through partnerships with major department stores or electronics retailers can be effective. We can also offer them an interest-free installment plans for larger purchases.
"""