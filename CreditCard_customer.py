# -*- coding: utf-8 -*-
"""Credit Card Customer Segmentation | Clustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xW-9swcNl75Pg4m52EVWwXZstxc-6wUy

# Data Import and Overview
"""

# Import the libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

data = pd.read_csv('/content/sample_data/CC GENERAL.csv')

data.head(10)

# Dataset has 8950 rows and 18 features

data.shape

# Some features have null-values. I have to take care of these rows before building a machine learning model for better performance.

data.info()

# By looking at the quick statistics, we can get broad insight about the dataset.

data.describe()

"""1. The average balance across 8950 customers are approximately $1564, reflecting the typical credit amount held in their accounts.

2. People tend to update their balance very often by approximately 88%.

3. The average purchase amount is about $1000 across all the customers.

4. Average single-time purchase by customers is $592, showing occasional substantial purchase behavior.

5. Installment-based purchases, averaging around $411, represent a significant portion of customer spending.

6. Cusomters tent to pay advance with average $980, indicating significant use of financial service.
"""

data.isnull().sum()

# No duplicated rows

data.duplicated().sum()

# Since we have relatively few number of rows for 'MINIMUM_PAYMENTS', just fill those rows with median values.

data['MINIMUM_PAYMENTS'] = data['MINIMUM_PAYMENTS'].fillna(data['MINIMUM_PAYMENTS'].median())

# Here, single row with null-values in 'CREDIT_LIMIT' is just dropped since it does not affect our analysis that much.

data.dropna(inplace=True)
data.isnull().sum()

# Also, 'CUST_ID' is non necessary for analysis since it has nothing important in its value.

data.drop(columns='CUST_ID', inplace=True)

"""## Histograms are used to look at the overview of trend and distribution of each features.

1. Common range of customer balance amount is 0 to $2500.
2. There are more people who either frequently make purchases or do not make purchases at all compared to those who make purchases on an average basis.
3. When using a credit card, most credit card customers prefer 12 months tenure compared to other tenure options. This can be concluded that customers are more likely to pay credits in the long term with the consequence of a higher interest rate.
4. Most of credit card accounts have 1 score in BALANCE_FREQUENCY column, which indicates that most customers use credit card frequently. However, this is different from ONEOFF_PURCHASES and PURCHASES_INSTALLMENT_FREQUENCY, where the majority of customers do not use credit cards for one-time transactions or payments in installments
"""

fig = plt.figure(figsize = (20,35))

for i, col in enumerate(data.columns):
  ax = plt.subplot(6, 3, i+1)
  sns.histplot(data, x = col, ax = ax, color = 'green', kde = True, alpha = 0.2)
  plt.xlabel(col, fontsize = 15)
  plt.ylabel("Frequency", fontsize = 15)
  plt.xticks(fontsize = 13)
  plt.yticks(fontsize = 13)
plt.tight_layout()
fig.show()

"""## Now generate box plots for each features to see the outliers

* Applying IQR methods on features with too many outliers and remove those to have better performance in our later model.
"""

columns_list = data.columns.tolist()

plt.rcParams['figure.figsize'] = [20,15]

sns.boxplot(data=data[columns_list], orient='v', palette='Set2', whis=1.5, saturation=1, width=0.7)
plt.title("Outliers Variable Distribution", fontsize = 14, fontweight = 'bold')
plt.ylabel("Range", fontweight = 'bold')
plt.xlabel("Attributes", fontweight = 'bold')
plt.xticks(rotation=90)

# Now, we have only 8588 rows after getting rid of outliers filtered by IQR method.

columns_to_process = ['BALANCE', 'PURCHASES', 'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE', 'CREDIT_LIMIT', 'PAYMENTS', 'MINIMUM_PAYMENTS']

for column in columns_to_process:
    Q1 = data[column].quantile(0.05)
    Q3 = data[column].quantile(0.95)
    IQR = Q3 - Q1
    data = data[(data[column] >= Q1 - 1.5*IQR) & (data[column] <= Q3 + 1.5*IQR)]

data.shape

"""## By looking at the heatmap, we can get hint how different features correlated each other.

1. People who frequently make many purchases often also tend to make numerous installment payments.
2. People with higher balances tend to have larger minimum payment amounts.
"""

corr_matrix = data.corr()
mask = np.abs(corr_matrix) < 0.5
fig = plt.figure(figsize=(15, 15))
sns.heatmap(corr_matrix, cmap='Reds', square=True, annot=True, linewidths=0.5, mask=mask)
plt.show()

# Filtering the columns that needed to be normalized

cols = ['BALANCE', 'PURCHASES', 'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE', 'CASH_ADVANCE_TRX', 'PURCHASES_TRX',
       'CREDIT_LIMIT', 'PAYMENTS', 'MINIMUM_PAYMENTS']

def feature_engi(column):
  column = np.sqrt(column)
  return column

for col in cols:
  data[col] = feature_engi(data[col])

"""##### **Normalization** is about adjusting the data distribution to reduce skewness and approximate a Gaussian distribution, which can be particularly beneficial for linear models and statistical analyses that assume normality.

##### **PCA** looks for directions that maximize variance. Without normalization, features with high variance and larger scales could dominate the principal components identified by PCA, regardless of their actual importance.
"""

# Let's check if the distibution of histogram is normally distributed.

fig = plt.figure(figsize = (20,35))
for i, col in enumerate(cols):
  ax = plt.subplot(6, 3, i+1)
  sns.histplot(data, x = col, ax = ax, color = 'blue', kde = True, alpha = 0.2)
  plt.xlabel(col, fontsize = 15)
  plt.ylabel("Frequency", fontsize = 15)
  plt.xticks(fontsize = 13)
  plt.yticks(fontsize = 13)
plt.tight_layout()
fig.show()

# Normalized dataset

data.head(5)

"""# Model Building"""

import sklearn
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer

"""##### Standardization focuses on scaling features to the same level, ensuring that each feature contributes equally to the algorithm's decision process, thereby enhancing the performance and stability of many machine learning models."""

scaler = StandardScaler()
df_scaled = scaler.fit_transform(data)

df_scaled = pd.DataFrame(data = df_scaled, columns = data.columns)
df_scaled.head()

"""# PCA (Principal Component Analysis) before Clustering

- Noise Reduction
- Computational Efficiency
- Interpretability
- Enhanced Cluster Separation
- Elimination of Redundancy
"""

pca = PCA()
X_pca = pca.fit_transform(df_scaled)
X_pca

df_pca = pd.DataFrame(data = X_pca, columns = [f'PC{i+1}' for i in range(X_pca.shape[1])])
df_pca.head()

evr = list(pca.explained_variance_ratio_)
evr = sorted(evr, reverse = True)

evr_cum = list(pca.explained_variance_ratio_.cumsum())

# number of PCA components

num_components = [x+1 for x in range(X_pca.shape[1])]
num_components

fig, ax = plt.subplots(1, 2, figsize = (12, 5))

sns.barplot(x = num_components, y = evr, ax = ax[0])
ax[0].set_xlabel("Component")
ax[0].set_title("% Explained Variance")

sns.lineplot(x = num_components, y = evr_cum, marker = 'o' ,ax = ax[1])
ax[1].set_xlabel("Component")
ax[1].set_title("% Cumulative Variance")

fig.show()

"""70-80% Cumulative Explained Variance: This is often considered a good balance for retaining most of the important information in the data while achieving significant dimensionality reduction.

I will go with **5 components** based on the above information.
"""

pca = PCA(n_components = 5)
X_pca = pca.fit_transform(df_scaled)

df_pca = pd.DataFrame(data = X_pca, columns = [f'PC{i+1}' for i in range(X_pca.shape[1])])
df_pca.head()

"""# Ready for KMeans Clustering

K-means clustering is unsupervised machine learning algorithm used for partitioning a dataset into a predefined number of clusters. The goal is to group data points that are similar to each other while minimizing the variance within each cluster.

Let's first investigate what is the optimal number of K by iterating the different k values from 1 to 10 in our model.

- After automated elbow method, it is concluded that k=3 is optimal number.
"""

kmeans_models = [KMeans(n_clusters = k, random_state = 42).fit(X_pca) for k in range(1,10)]

inertias = [model.inertia_ for model in kmeans_models]

fig = plt.figure(figsize = (6,4))
sns.lineplot(x = list(range(1,10)), y = inertias, marker = 'x')
plt.xlabel("Number of Clusters")
plt.ylabel('Within-Cluster Sum of Squares(Inertia)')
plt.show()

kmeans = KMeans()
vis_elbow = KElbowVisualizer(kmeans, k = (1, 10))
vis_elbow.fit(df_scaled)
vis_elbow.poof()

kmeans = KMeans(n_clusters = 3, random_state=0)

pca_labels = kmeans.fit_predict(X_pca)

plt.figure(figsize=(25,15))
sns.scatterplot(data = df_pca, x= "PC1", y= "PC2", hue = pca_labels, palette='viridis')
plt.xlabel("PC1", fontsize = 25)
plt.ylabel("PC2", fontsize = 25)
plt.legend(['Cluster 0', 'Cluster 1', 'Cluster 2'],fontsize = 25)

plt.show()

pca_labels

# reverting the feature values by squaring the values

def feature_engg(column):
    column = np.square(column)
    return column

for col in cols:
    data[col] = data[col].apply(feature_engg)

data['cluster'] = pca_labels
data.head()

"""# Cluster Analysis"""

sns.set(style="whitegrid")
palette = sns.color_palette("Set1")

ax = sns.countplot(x='cluster', data=data, palette=palette, label='Count')

for p in ax.patches:
    ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center',
                xytext=(0, 5), textcoords='offset points')

plt.show()

data.groupby('cluster').mean()

"""## Cluster 0:

신용카드 빌린 금액 높으며 잦다, 구매 빈도가 낮다. 카드대출을 이미 많이 받았으며 대출 상환율이 매우 낮다.

- High balance with fairly frequent update
- Few purchases (one-off and installments)
- Cash advances with higher amounts
- Rarely pay the full amount of their credit card bill

For this group, offering products that incentivize full payments, such as cashback rewards, could be effective. Additionally, promoting low-interest rates on cash advances or providing offers that encourage more frequent use of the card for purchases could increase engagement.

## Cluster 1:

신용카드 사용빈도가 낮고, 대다수의 할부로 잦은 구매하는 편. 카드 대출을 자주 쓰지는 않으며 돈을 잘 갚는다. 즉, 신용점수에 민감한 그룹.

- Low balance with less frequent update
- Moderate purchases with significant portion in installments
- Frequent purchase
- Rare cash advancement
- Higher full payment rate

For this group, offering products that incentivize full payments, such as cashback rewards, could be effective. Additionally, promoting low-interest rates on cash advances or providing offers that encourage more frequent use of the card for purchases could increase engagement.

## Cluster 2:

잦은 신용카드 사용, 특히 할부 이용해 구매 빈도가 높다. 카드대출도 받는편이며 대출을 나름 잘 갚는편.

- Moderate to High balance with frequent balance update
- High purchases (especially installments)
- Moderate Cash advances
- Moderate pay the full amount of their credit card bill

Customers in this cluster might respond well to rewards that benefit frequent and high spenders, such as tiered rewards programs. Offers that encourage larger purchases or bonus points for installment purchases could also be appealing. Additionally, providing incentives for paying off balances could encourage more full payments.
"""